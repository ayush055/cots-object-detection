{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-01-19T23:33:13.717781Z","iopub.status.busy":"2022-01-19T23:33:13.717504Z","iopub.status.idle":"2022-01-19T23:33:16.136341Z","shell.execute_reply":"2022-01-19T23:33:16.135591Z","shell.execute_reply.started":"2022-01-19T23:33:13.717742Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","from tqdm.notebook import tqdm\n","tqdm.pandas()\n","import pandas as pd\n","import os\n","import cv2\n","import matplotlib.pyplot as plt\n","import glob\n","import shutil\n","import sys\n","import torch\n","from sklearn.model_selection import GroupKFold\n","from PIL import Image\n","import ast\n","import sahi\n","from sahi.predict import get_sliced_prediction"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ“Œ Key-Points\n","* One have to submit prediction using the provided **python time-series API**, which makes this competition different from previous Object Detection Competitions.\n","* Each prediction row needs to include all bounding boxes for the image. Submission is format seems also **COCO** which means `[x_min, y_min, width, height]`\n","* Copmetition metric `F2` tolerates some false positives(FP) in order to ensure very few starfish are missed. Which means tackling **false negatives(FN)** is more important than false positives(FP). \n","$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$"]},{"cell_type":"markdown","metadata":{},"source":["## Please Upvote if you find this Helpful"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ“– Meta Data\n","* `train_images/` - Folder containing training set photos of the form `video_{video_id}/{video_frame}.jpg`.\n","\n","* `[train/test].csv` - Metadata for the images. As with other test files, most of the test metadata data is only available to your notebook upon submission. Just the first few rows available for download.\n","\n","* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n","* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n","* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n","* `sequence_frame` - The frame number within a given sequence.\n","* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n","* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format)."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:16.138884Z","iopub.status.busy":"2022-01-19T23:33:16.138603Z","iopub.status.idle":"2022-01-19T23:33:16.14327Z","shell.execute_reply":"2022-01-19T23:33:16.142438Z","shell.execute_reply.started":"2022-01-19T23:33:16.138849Z"},"trusted":true},"outputs":[],"source":["ROOT_DIR  = './tensorflow-great-barrier-reef'\n","CKPT_PATH = './yolov5/runs/train/exp67/weights/best.pt'\n","IMG_SIZE  = 2560\n","CONF      = 0.01\n","IOU       = 0.6\n","AUGMENT   = False"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:16.144937Z","iopub.status.busy":"2022-01-19T23:33:16.144521Z","iopub.status.idle":"2022-01-19T23:33:16.153732Z","shell.execute_reply":"2022-01-19T23:33:16.152977Z","shell.execute_reply.started":"2022-01-19T23:33:16.144898Z"},"trusted":true},"outputs":[],"source":["def get_path(row):\n","    row['image_path'] = f'{ROOT_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n","    return row"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:16.155785Z","iopub.status.busy":"2022-01-19T23:33:16.155138Z","iopub.status.idle":"2022-01-19T23:33:31.93096Z","shell.execute_reply":"2022-01-19T23:33:31.930043Z","shell.execute_reply.started":"2022-01-19T23:33:16.155755Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e758ef1e860468b805c8dbfe2e31104","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/23501 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83cd5b04158846a691dee8713e435e60","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/23501 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","      <th>sequence</th>\n","      <th>video_frame</th>\n","      <th>sequence_frame</th>\n","      <th>image_id</th>\n","      <th>annotations</th>\n","      <th>image_path</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0-0</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0-1</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n","0         0     40258            0               0      0-0          []   \n","1         0     40258            1               1      0-1          []   \n","\n","                                          image_path  \n","0  ./tensorflow-great-barrier-reef/train_images/v...  \n","1  ./tensorflow-great-barrier-reef/train_images/v...  "]},"metadata":{},"output_type":"display_data"}],"source":["# Train Data\n","df = pd.read_csv(f'{ROOT_DIR}/train.csv')\n","df = df.progress_apply(get_path, axis=1)\n","df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n","display(df.head(2))"]},{"cell_type":"markdown","metadata":{},"source":["## Number of BBoxes"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:31.932959Z","iopub.status.busy":"2022-01-19T23:33:31.932674Z","iopub.status.idle":"2022-01-19T23:33:32.033135Z","shell.execute_reply":"2022-01-19T23:33:32.032419Z","shell.execute_reply.started":"2022-01-19T23:33:31.932919Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e54b13bee43b4622aeacf2f0fcef814b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/23501 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["No BBox: 79.07% | With BBox: 20.93%\n"]}],"source":["df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n","data = (df.num_bbox>0).value_counts()/len(df)*100\n","print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:32.034835Z","iopub.status.busy":"2022-01-19T23:33:32.034294Z","iopub.status.idle":"2022-01-19T23:33:32.067973Z","shell.execute_reply":"2022-01-19T23:33:32.067107Z","shell.execute_reply.started":"2022-01-19T23:33:32.034796Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>video_id</th>\n","      <th>sequence</th>\n","      <th>video_frame</th>\n","      <th>sequence_frame</th>\n","      <th>image_id</th>\n","      <th>annotations</th>\n","      <th>image_path</th>\n","      <th>num_bbox</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0-0</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0-1</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0-2</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0-3</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>40258</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>0-4</td>\n","      <td>[]</td>\n","      <td>./tensorflow-great-barrier-reef/train_images/v...</td>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n","0         0     40258            0               0      0-0          []   \n","1         0     40258            1               1      0-1          []   \n","2         0     40258            2               2      0-2          []   \n","3         0     40258            3               3      0-3          []   \n","4         0     40258            4               4      0-4          []   \n","\n","                                          image_path  num_bbox  fold  \n","0  ./tensorflow-great-barrier-reef/train_images/v...         0     2  \n","1  ./tensorflow-great-barrier-reef/train_images/v...         0     2  \n","2  ./tensorflow-great-barrier-reef/train_images/v...         0     2  \n","3  ./tensorflow-great-barrier-reef/train_images/v...         0     2  \n","4  ./tensorflow-great-barrier-reef/train_images/v...         0     2  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["NUM_FOLDS = 3\n","kf = GroupKFold(n_splits = NUM_FOLDS)\n","df = df.reset_index(drop=True)\n","df['fold'] = -1\n","for fold, (train_idx, val_idx) in enumerate(kf.split(df, y = df.video_id.tolist(), groups=df.video_id.tolist())):\n","    df.loc[val_idx, 'fold'] = fold\n","\n","df.head(5)"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ”¨ Helper"]},{"cell_type":"code","execution_count":7,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-20T00:07:55.204226Z","iopub.status.busy":"2022-01-20T00:07:55.203967Z","iopub.status.idle":"2022-01-20T00:07:55.243831Z","shell.execute_reply":"2022-01-20T00:07:55.242861Z","shell.execute_reply.started":"2022-01-20T00:07:55.204197Z"},"trusted":true},"outputs":[],"source":["def voc2yolo(bboxes, image_height=720, image_width=1280):\n","    \"\"\"\n","    voc  => [x1, y1, x2, y1]\n","    yolo => [xmid, ymid, w, h] (normalized)\n","    \"\"\"\n","    \n","    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n","    \n","    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n","    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n","    \n","    w = bboxes[..., 2] - bboxes[..., 0]\n","    h = bboxes[..., 3] - bboxes[..., 1]\n","    \n","    bboxes[..., 0] = bboxes[..., 0] + w/2\n","    bboxes[..., 1] = bboxes[..., 1] + h/2\n","    bboxes[..., 2] = w\n","    bboxes[..., 3] = h\n","    \n","    return bboxes\n","\n","def yolo2voc(bboxes, image_height=720, image_width=1280):\n","    \"\"\"\n","    yolo => [xmid, ymid, w, h] (normalized)\n","    voc  => [x1, y1, x2, y1]\n","    \n","    \"\"\" \n","    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n","    \n","    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n","    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n","    \n","    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n","    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n","    \n","    return bboxes\n","\n","def coco2yolo(bboxes, image_height=720, image_width=1280):\n","    \"\"\"\n","    coco => [xmin, ymin, w, h]\n","    yolo => [xmid, ymid, w, h] (normalized)\n","    \"\"\"\n","    \n","    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n","    \n","    # normolizinig\n","    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n","    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n","    \n","    # converstion (xmin, ymin) => (xmid, ymid)\n","    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n","    \n","    return bboxes\n","\n","def yolo2coco(bboxes, image_height=720, image_width=1280):\n","    \"\"\"\n","    yolo => [xmid, ymid, w, h] (normalized)\n","    coco => [xmin, ymin, w, h]\n","    \n","    \"\"\" \n","    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n","    \n","    # denormalizing\n","    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n","    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n","    \n","    # converstion (xmid, ymid) => (xmin, ymin) \n","    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n","    \n","    return bboxes\n","\n","def voc2coco(bboxes, image_height=720, image_width=1280):\n","    bboxes  = voc2yolo(bboxes, image_height, image_width)\n","    bboxes  = yolo2coco(bboxes, image_height, image_width)\n","    return bboxes\n","\n","\n","def load_image(image_path):\n","    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n","\n","\n","def plot_one_box(x, img, color=None, label=None, line_thickness=None, conf=None):\n","    # Plots one bounding box on image img\n","    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n","    color = color or [random.randint(0, 255) for _ in range(3)]\n","    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n","    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n","    if label:\n","        tf = max(tl - 1, 1)  # font thickness\n","        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n","        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n","        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n","        if conf:\n","            print(conf)\n","            cv2.putText(img, str(conf), (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","#         else:\n","#             cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n","\n","def draw_bboxes(img, bboxes, classes, class_ids, confs = None, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n","     \n","    image = img.copy()\n","    show_classes = classes if show_classes is None else show_classes\n","    colors = (0, 255 ,0) if colors is None else colors\n","    \n","    if bbox_format == 'yolo':\n","        \n","        for idx in range(len(bboxes)):  \n","            \n","            bbox  = bboxes[idx]\n","            cls   = classes[idx]\n","            cls_id = class_ids[idx]\n","            color = colors[cls_id] if type(colors) is list else colors\n","            \n","            if cls in show_classes:\n","            \n","                x1 = round(float(bbox[0])*image.shape[1])\n","                y1 = round(float(bbox[1])*image.shape[0])\n","                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n","                h  = round(float(bbox[3])*image.shape[0]/2)\n","\n","                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n","                plot_one_box(voc_bbox, \n","                             image,\n","                             color = color,\n","                             label = cls if class_name else str(get_label(cls)),\n","                             line_thickness = line_thickness,\n","                             conf = confs[idx])\n","            \n","    elif bbox_format == 'coco':\n","        \n","        for idx in range(len(bboxes)):  \n","            \n","            bbox  = bboxes[idx]\n","            cls   = classes[idx]\n","            cls_id = class_ids[idx]\n","            color = colors[cls_id] if type(colors) is list else colors\n","            \n","            if cls in show_classes:            \n","                x1 = int(round(bbox[0]))\n","                y1 = int(round(bbox[1]))\n","                w  = int(round(bbox[2]))\n","                h  = int(round(bbox[3]))\n","\n","                voc_bbox = (x1, y1, x1+w, y1+h)\n","                plot_one_box(voc_bbox, \n","                             image,\n","                             color = color,\n","                             label = cls if class_name else str(cls_id),\n","                             line_thickness = line_thickness,\n","                             conf = confs[idx])\n","\n","    elif bbox_format == 'voc_pascal':\n","        \n","        for idx in range(len(bboxes)):  \n","            \n","            bbox  = bboxes[idx]\n","            cls   = classes[idx]\n","            cls_id = class_ids[idx]\n","            color = colors[cls_id] if type(colors) is list else colors\n","            \n","            if cls in show_classes: \n","                x1 = int(round(bbox[0]))\n","                y1 = int(round(bbox[1]))\n","                x2 = int(round(bbox[2]))\n","                y2 = int(round(bbox[3]))\n","                voc_bbox = (x1, y1, x2, y2)\n","                plot_one_box(voc_bbox, \n","                             image,\n","                             color = color,\n","                             label = cls if class_name else str(cls_id),\n","                             line_thickness = line_thickness)\n","    else:\n","        raise ValueError('wrong bbox format')\n","    \n","    return image\n","\n","def get_bbox(annots):\n","    bboxes = [list(annot.values()) for annot in annots]\n","    return bboxes\n","\n","def get_imgsize(row):\n","    row['width'], row['height'] = imagesize.get(row['image_path'])\n","    return row\n","\n","np.random.seed(32)\n","colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n","          for idx in range(1)]"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-19T23:33:32.110247Z","iopub.status.busy":"2022-01-19T23:33:32.109828Z","iopub.status.idle":"2022-01-19T23:33:33.516898Z","shell.execute_reply":"2022-01-19T23:33:33.515825Z","shell.execute_reply.started":"2022-01-19T23:33:32.110205Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The syntax of the command is incorrect.\n","'cp' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!mkdir -p /root/.config/Ultralytics\n","!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/"]},{"cell_type":"code","execution_count":9,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-19T23:33:33.519202Z","iopub.status.busy":"2022-01-19T23:33:33.518781Z","iopub.status.idle":"2022-01-19T23:33:33.526711Z","shell.execute_reply":"2022-01-19T23:33:33.525896Z","shell.execute_reply.started":"2022-01-19T23:33:33.519163Z"},"trusted":true},"outputs":[],"source":["def load_model(ckpt_path, conf=0.25, iou=0.50):\n","    model = torch.hub.load('./yolov5',\n","                           'custom',\n","                           path=ckpt_path,\n","                           source='local',\n","                           force_reload=True)  # local repo\n","    model.conf = conf  # NMS confidence threshold\n","    model.iou  = iou  # NMS IoU threshold\n","    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n","    model.multi_label = False  # NMS multiple labels per box\n","    model.max_det = 1000  # maximum number of detections per image\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ”­ Inference"]},{"cell_type":"markdown","metadata":{},"source":["## Helper"]},{"cell_type":"code","execution_count":10,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-20T00:02:49.097626Z","iopub.status.busy":"2022-01-20T00:02:49.097119Z","iopub.status.idle":"2022-01-20T00:02:49.112068Z","shell.execute_reply":"2022-01-20T00:02:49.111211Z","shell.execute_reply.started":"2022-01-20T00:02:49.097581Z"},"trusted":true},"outputs":[],"source":["def predict(model, img, size=1280, augment=False):\n","    height, width = img.shape[:2]\n","    results = model(img, size=size, augment=augment)  # custom inference size\n","    preds   = results.pandas().xyxy[0]\n","    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n","    if len(bboxes):\n","        bboxes  = voc2coco(bboxes,height,width).astype(int)\n","        confs   = preds.confidence.values\n","        return bboxes, confs\n","    else:\n","        return [],[]\n","    \n","def format_prediction(bboxes, confs):\n","    annot = ''\n","    if len(bboxes)>0:\n","        for idx in range(len(bboxes)):\n","            xmin, ymin, w, h = bboxes[idx]\n","            conf             = confs[idx]\n","            annot += f'{conf} {xmin} {ymin} {w} {h}'\n","            annot +=' '\n","        annot = annot.strip(' ')\n","    return annot\n","\n","def show_img(img, bboxes, bboxes_true=False, scores=None, bbox_format='yolo'):\n","    names  = ['starfish']*len(bboxes)\n","    labels = [0]*len(bboxes)\n","    img    = draw_bboxes(img = img,\n","                           bboxes = bboxes, \n","                           classes = names,\n","                           class_ids = labels,\n","                           confs = scores,\n","                           class_name = True, \n","                           colors = (0, 255, 0), \n","                           bbox_format = bbox_format,\n","                           line_thickness = 2)\n","    if bboxes_true:\n","        names_true  = ['starfish']*len(bboxes_true)\n","        labels_true = [0]*len(bboxes_true)\n","        img = draw_bboxes(img = img,\n","                               bboxes = bboxes_true, \n","                               classes = names_true,\n","                               class_ids = labels_true,\n","                               class_name = True, \n","                               colors = (255, 0, 0), \n","                               bbox_format = bbox_format,\n","                               line_thickness = 2)\n","    return Image.fromarray(img).resize((800, 400))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:33.550908Z","iopub.status.busy":"2022-01-19T23:33:33.550372Z","iopub.status.idle":"2022-01-19T23:33:33.560112Z","shell.execute_reply":"2022-01-19T23:33:33.559504Z","shell.execute_reply.started":"2022-01-19T23:33:33.550872Z"},"trusted":true},"outputs":[],"source":["sharp_filter = np.array([[0,-1,0], [-1,5,-1], [0,-1,0]])\n","def gamma_enhance(img, gamma=1.0):\n","    lookUpTable = np.empty((1,256), np.uint8)\n","    for i in range(256):\n","        lookUpTable[0,i] = np.clip(pow(i / 255.0, gamma) * 255.0, 0, 255)\n","    return cv2.LUT(img, lookUpTable)"]},{"cell_type":"markdown","metadata":{},"source":["## Run Inference on **Train**"]},{"cell_type":"code","execution_count":12,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-19T23:33:33.562064Z","iopub.status.busy":"2022-01-19T23:33:33.561611Z","iopub.status.idle":"2022-01-19T23:33:33.571095Z","shell.execute_reply":"2022-01-19T23:33:33.570426Z","shell.execute_reply.started":"2022-01-19T23:33:33.562027Z"},"trusted":true},"outputs":[],"source":["# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n","# image_paths = df[df.num_bbox>1].iloc[0:500:100].image_path.tolist()\n","# for idx, path in enumerate(image_paths):\n","#     img = cv2.imread(path)[...,::-1]\n","#     bboxes, confis = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n","#     display(show_img(img, bboxes, bbox_format='coco'))\n","#     if idx>5:\n","#         break"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:33:33.572748Z","iopub.status.busy":"2022-01-19T23:33:33.57233Z","iopub.status.idle":"2022-01-19T23:33:33.582106Z","shell.execute_reply":"2022-01-19T23:33:33.581393Z","shell.execute_reply.started":"2022-01-19T23:33:33.572712Z"},"trusted":true},"outputs":[],"source":["from sahi.model import Yolov5DetectionModel\n","from sahi.predict import get_prediction, get_sliced_prediction, predict"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:49:37.008867Z","iopub.status.busy":"2022-01-19T23:49:37.008051Z","iopub.status.idle":"2022-01-19T23:49:37.030664Z","shell.execute_reply":"2022-01-19T23:49:37.029891Z","shell.execute_reply.started":"2022-01-19T23:49:37.008804Z"},"trusted":true},"outputs":[],"source":["annotations = df[df['annotations'].str.len()>1].annotations"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:52:56.978509Z","iopub.status.busy":"2022-01-19T23:52:56.978207Z","iopub.status.idle":"2022-01-19T23:52:57.004701Z","shell.execute_reply":"2022-01-19T23:52:57.00333Z","shell.execute_reply.started":"2022-01-19T23:52:56.978455Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Average bbox area: 2198.0414422337035 Average Width: 47.68923821039903 Average height: 42.65362207321095\n","Max width and height: 141 126\n"]}],"source":["area = 0\n","count = 0\n","width = 0\n","height = 0\n","max_w = 0\n","max_h = 0\n","for frame in annotations:\n","    for box in frame:\n","        area += box['width'] * box['height']\n","        width += box['width']\n","        height += box['height']\n","        max_w = max(box['width'], max_w)\n","        max_h = max(box['height'], max_h)\n","        count += 1\n","print(\"Average bbox area:\", area/count, \"Average Width:\", width/count, \"Average height:\", height/count)\n","print(\"Max width and height:\", max_w, max_h)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-01-20T00:07:57.800076Z","iopub.status.busy":"2022-01-20T00:07:57.799637Z","iopub.status.idle":"2022-01-20T00:08:00.600986Z","shell.execute_reply":"2022-01-20T00:08:00.599933Z","shell.execute_reply.started":"2022-01-20T00:07:57.800036Z"},"trusted":true},"outputs":[{"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'names'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19544/3191870864.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m detection_model = Yolov5DetectionModel(\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mCKPT_PATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mconfidence_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Ayush\\github\\cots-object-detection\\cots2\\lib\\site-packages\\sahi\\model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model_path, config_path, device, mask_threshold, confidence_threshold, category_mapping, category_remapping, load_at_init, image_size)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# automatically load model if load_at_init is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload_at_init\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Ayush\\github\\cots-object-detection\\cots2\\lib\\site-packages\\sahi\\model.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;31m# set category_mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory_mapping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[0mcategory_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcategory_name\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory_mapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcategory_mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Ayush\\github\\cots-object-detection\\cots2\\lib\\site-packages\\sahi\\model.py\u001b[0m in \u001b[0;36mcategory_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcategory_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     def _create_object_prediction_list_from_original_predictions(\n","\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'names'"]}],"source":["# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n","detection_model = Yolov5DetectionModel(\n","    model_path=CKPT_PATH,\n","    confidence_threshold=0.15,\n","    device=\"cuda:0\"\n",")\n","\n","image_paths = df[df.num_bbox>1].iloc[0:500:100].image_path.tolist()\n","image_paths.append(df.iloc[12716].image_path)\n","\n","for idx, path in enumerate(image_paths):\n","    bboxes_true = [list(box.values()) for box in df[df.image_path == path].annotations.values[0]]\n","    result = get_sliced_prediction(\n","    path,\n","    detection_model,\n","    image_size = 2560,\n","    slice_height = 256,\n","    slice_width = 256,\n","    overlap_height_ratio = 0.2,\n","    overlap_width_ratio = 0.2\n",")\n","    bboxes = [pred.bbox.to_coco_bbox() for pred in result.object_prediction_list]\n","    scores = [pred.score.value for pred in result.object_prediction_list]\n","    img = np.array(result.image)\n","    display(show_img(img, bboxes, bboxes_true, scores, bbox_format='coco'))\n","    print(scores)\n","\n","#     break\n","#     img = cv2.imread(path)[...,::-1]\n","#     result = get_sliced_prediction(\n","#     img,\n","#     model,\n","#     slice_height = 256,\n","#     slice_width = 256,\n","#     overlap_height_ratio = 0.2,\n","#     overlap_width_ratio = 0.2\n","# )\n","#     display(show_img(img, bboxes, bbox_format='coco'))\n","#     if idx>5:\n","#         break"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-19T23:55:48.639971Z","iopub.status.busy":"2022-01-19T23:55:48.639444Z","iopub.status.idle":"2022-01-19T23:55:58.279605Z","shell.execute_reply":"2022-01-19T23:55:58.278965Z","shell.execute_reply.started":"2022-01-19T23:55:48.639931Z"},"trusted":true},"outputs":[],"source":["# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n","\n","df_temp = df[df['fold'] == 0]\n","image_paths = df_temp[df_temp.num_bbox>1].iloc[0:500:100].image_path.tolist()\n","# image_paths.append(df.iloc[12716].image_path)\n","\n","for idx, path in enumerate(image_paths):\n","    bboxes_true = [list(box.values()) for box in df_temp[df_temp.image_path == path].annotations.values[0]]\n","    result = get_sliced_prediction(\n","    path,\n","    detection_model,\n","    image_size = 2560,\n","    slice_height = 256,\n","    slice_width = 256,\n","    overlap_height_ratio = 0.2,\n","    overlap_width_ratio = 0.2\n",")\n","    bboxes = [pred.bbox.to_coco_bbox() for pred in result.object_prediction_list]\n","    scores = [pred.score.value for pred in result.object_prediction_list]\n","    img = np.array(result.image)\n","    display(show_img(img, bboxes, bboxes_true, bbox_format='coco'))\n","    print(scores)\n","\n","#     break\n","#     img = cv2.imread(path)[...,::-1]\n","#     result = get_sliced_prediction(\n","#     img,\n","#     model,\n","#     slice_height = 256,\n","#     slice_width = 256,\n","#     overlap_height_ratio = 0.2,\n","#     overlap_width_ratio = 0.2\n","# )\n","#     display(show_img(img, bboxes, bbox_format='coco'))\n","#     if idx>5:\n","#         break"]},{"cell_type":"markdown","metadata":{},"source":["## Init `Env`"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-15T23:51:19.04644Z","iopub.status.busy":"2022-01-15T23:51:19.045774Z","iopub.status.idle":"2022-01-15T23:51:19.077958Z","shell.execute_reply":"2022-01-15T23:51:19.077255Z","shell.execute_reply.started":"2022-01-15T23:51:19.046401Z"},"trusted":true},"outputs":[],"source":["import greatbarrierreef\n","env = greatbarrierreef.make_env()# initialize the environment\n","iter_test = env.iter_test()      # an iterator which loops over the test set and sample submission"]},{"cell_type":"markdown","metadata":{},"source":["## Run Inference on **Test**"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-01-15T23:51:19.079778Z","iopub.status.busy":"2022-01-15T23:51:19.079514Z","iopub.status.idle":"2022-01-15T23:51:19.085401Z","shell.execute_reply":"2022-01-15T23:51:19.084694Z","shell.execute_reply.started":"2022-01-15T23:51:19.079742Z"},"trusted":true},"outputs":[],"source":["# model = load_model(CKPT_PATH, conf=CONF, iou=IOU)\n","# for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n","#     bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n","#     annot          = format_prediction(bboxes, confs)\n","#     pred_df['annotations'] = annot\n","#     env.predict(pred_df)\n","# #     if idx<3:\n","# #         display(show_img(img, bboxes, bbox_format='coco'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-15T23:51:19.087303Z","iopub.status.busy":"2022-01-15T23:51:19.086783Z","iopub.status.idle":"2022-01-15T23:51:35.243435Z","shell.execute_reply":"2022-01-15T23:51:35.242677Z","shell.execute_reply.started":"2022-01-15T23:51:19.087264Z"},"trusted":true},"outputs":[],"source":["detection_model = Yolov5DetectionModel(\n","    model_path=CKPT_PATH,\n","    confidence_threshold=0.1,\n","    device=\"cuda:0\"\n",")\n","\n","for idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n","    result = get_sliced_prediction(\n","    img,\n","    detection_model,\n","#     image_size = 3600,\n","    slice_height = 256,\n","    slice_width = 256,\n","    overlap_height_ratio = 0.2,\n","    overlap_width_ratio = 0.2\n",")\n","    bboxes = [pred.bbox.to_coco_bbox() for pred in result.object_prediction_list]\n","    confs = [pred.score.value for pred in result.object_prediction_list]\n","    annot          = format_prediction(bboxes, confs)\n","    pred_df['annotations'] = annot\n","    env.predict(pred_df)\n","    if idx<3:\n","        display(show_img(img, bboxes, bbox_format='coco'))"]},{"cell_type":"markdown","metadata":{},"source":["# ðŸ‘€ Check Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-01-15T23:51:35.245206Z","iopub.status.busy":"2022-01-15T23:51:35.244785Z","iopub.status.idle":"2022-01-15T23:51:35.258376Z","shell.execute_reply":"2022-01-15T23:51:35.257337Z","shell.execute_reply.started":"2022-01-15T23:51:35.245172Z"},"trusted":true},"outputs":[],"source":["sub_df = pd.read_csv('submission.csv')\n","sub_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Please Upvote if you find this Helpful"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://www.pngall.com/wp-content/uploads/2018/04/Under-Construction-PNG-File.png\">"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":4}
